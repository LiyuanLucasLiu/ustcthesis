\begin{abstract}

在自然语言处理（Natural Language Processing）领域，如何能合理地表示词语是一个全世界学者广泛关注的基础性问题。近几年来，一系列词向量嵌入的模型（Word Embedding），如由Google提出的\emph{word2vec}模型，在这一问题上取得了突破性的进展，也使得词向量嵌入模型成为了研究的热点。本文先对现有的词向量嵌入模型进行了总结与分析、讨论了词向量的性质，然后基于这些模型的一些缺点, 如无法很好地判断和处理多义词，对模型进行了改进，最后通过实验对改进后的模型进行了验证，说明了改进的有效性。

\keywords{自然语言处理\zhspace{} 词向量嵌入\zhspace{} 多义词}
\end{abstract}

\begin{enabstract}

The task of representing words, known as Word Representation or Word Embedding, is one of the major topics in the field of Natural Language Processing. With the recent advances like the \emph{word2vec} toolbox, models and algorighms to represente words or phrases as vectors of real numbers in a low-dimensional space received a significant amount of attention. In this paper, we summarize and analyze several popular word embedding models, improve word representation to overcome several drawbacks, such as the lack of method to judge the number of polysemy's different semantic meanings. Extensive experimental results demonstrates the effective of proposed method.

\enkeywords{Natural Language Processing, Word Embedding, Polysemy}
\end{enabstract}
