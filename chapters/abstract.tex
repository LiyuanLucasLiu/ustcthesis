\begin{abstract}

在自然语言处理（Natural Language Processing）领域，一个基础性的受到全世界学者广泛关注的问题是如何能合理地表示词语。近几年来，\emph{word2vec}及其相关的一系列工作通过词向量嵌入的方法（Word Embedding）在这一问题上取得了突破性的成果，也使得词向量嵌入的模型成为了研究的热点。本文对现有的词向量嵌入模型进行了总结与分析，对词向量的性质进行了分析与讨论，然后基于这些模型的一些缺点, 如无法很好地判断和处理多义词，对模型进行了改进，并通过实验对改进后的模型进行了验证，说明了改进的有效性。

\keywords{自然语言处理\zhspace{} 词向量嵌入\zhspace{} 多义词}
\end{abstract}

\begin{enabstract}

The task of representing words, known as Word Representation or Word Embedding, is one of the major topics in Natural Language Processing. With the recent advances like the \emph{word2vec} toolbox, how to representing words or phrases as vectors of real numbers in a low-dimensional space has received a significant amount of attention. In this paper, we summarize and analyze several popular word embedding models, improve word representation to overcome former drawbacks, such as the lack of method to judge and deal with polysemy. Extensive experimental results demonstrates the effective of proposed method.

\enkeywords{Natural Language Processing, Word Embedding, Polysemy}
\end{enabstract}
