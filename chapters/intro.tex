\iffalse

\bibliography{..\\bib\\tex.bib}

\fi

\chapter{引言}
\label{chap:intro}
文本数据是机器学习，数据挖掘，自然语言处理等领域所经常要面对的一种数据形式。在处理文本数据时，常常所要面对的一个问题就是应该如何表示词语。常用到一种方法是，使用一种只有一个维度的值为1其余维度的值均为0的向量来表示值为1的维度的编号所对应的词语，例如，使用$[1, 0, ..., 0]$表示字典中的第一个词语（这种向量也被称为one-hot 向量）。这种方法非常直观，但由于字典中往往会包括多达几万的不同词语，one-hot向量往往会有较大的维度，而使得模型的训练变得非常困难。另一方面，这种方法，如同大多数传统的方法一样，忽略了词语的语义和语法层面的信息与关系。

\citep{rumelhart1988learning}第一次提出了词向量的概念，这是一种类似one-hot向量用一一对应的向量表示词语的方法，但与one-hot向量所不同的是，词向量没有将向量限制为某种特殊的向量，而是使用维度不高的实数空间上的一般的向量来表示词语。这种做法使得表达词语的向量维度可以被大大缩小。同时，由于词向量不再由人工设定的方式生成，而是根据文本的特点通过学习自动生成，通过这种方式生成的向量，往往也会携带一部分词语的语义与语法信息。由于这些特性，在词向量的概念被提出后，迅速被广泛运用到神经网络语言模型等许多自然语言处理的问题中，并取得了可观的成功\citep{collobert2008unified,schwenk2007continuous}。

几年前，Google 发布了著名的\emph{word2vec}工具包，这个工具包训练得到的词向量不光具有维度低的特点，还具有一些非常有趣的特性，这些性质使得词向量学习成为了近几年研究的热点\citep{mikolov2013efficient,mikolov2013distributed,mikolov2013linguistic}。一方面，\emph{word2vec}的一个特点是它可以在较大量的语料上以较高效率学习并获得具有较高质量的词向量，这使得它可以被广泛得运用在各种模型中。另一方面，更引人注意的是，通过这个模型学习出的词向量可以通过向量的加减运算来表示词语间的语义或语法关系。比如说，如果我们用$\vvec[\mbox{词语}]$表示一个词语所对应的词向量，我们会发现$\vvec[\mbox{man}]-\vvec[\mbox{woman}] \approx \vvec[\mbox{king}] - \vvec[\mbox{queen}]$，$\vvec[\mbox{Chinese}] + \vvec[\mbox{river}] \approx \vvec[\mbox{Yangtze\_River}]$，即，男人与女人所对应的词向量之差会近似于国王与王后对应词向量之差，中国与河流对应词向量之和会近似于长江的词向量，而男人与女人的关系与国王与王后的关系确实是非常相似的，长江也可以看做中国和河流两个词语词义的叠加的近似。这一现象表示\emph{word2vec} 所学习获得的词向量是可以较好得表示词语之间语义或语法的关系的。

由于这一特性，在\emph{word2vec}提出后，迅速出现了许多对这一模型进行解释、改进的研究工作\citep{li2015word,levy2014neural,pennington2014glove,huang2012improving}。这些工作中，有些对\emph{word2vec}的数学原理进行了探讨，为\emph{word2vec}方法提供了理论的依据，例如\citep{li2015word,levy2014neural}证明\emph{word2vec}的学习过程近似等价于对由词语同时出现的次数所的构成的某种信息矩阵进行矩阵分解。另外一部分工作\emph{word2vec}所学习得到的词向量之间的关系可以通过代数运算表示的原因，并在此基础上提出了新的模型\citep{pennington2014glove}。还有一部分工作，并针对多义词现象对\emph{word2vec}的模型进行了改进与强化\citep{huang2012improving}。

下面，我们先对之前的一部分经典工作进行总结，然后再在他们的基础上进一步解释为什么可以使用词向量间的代数运算表示词语语义与语法关系，同时也针对多义词的现象进一步对词向量学习的算法与模型进行改进，并通过实验验证了这些改进的有效性与可行性。