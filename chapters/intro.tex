\iffalse

\bibliography{..\\bib\\tex.bib}

\fi

\chapter{引言}
\label{chap:intro}
文本数据是机器学习，数据挖掘，自然语言处理等领域所经常需要处理的一种数据形式。在处理文本数据时，常常所要面对的一个问题就是应该如何表示词语。为了解决这个问题，一种常用的方法是，使用一种只有一个维度的值为1其余维度的值均为0的向量来表示值为1的维度的编号所对应的词语，例如，使用$[1, 0, ..., 0]$表示字典中的第一个词语（这种向量也被称为one-hot 向量）。这种方法非常直观，但由于字典中往往会包括多达几万个不同词语，one-hot向量往往会有较大的维度，而使得模型的训练变得较为困难。另一方面，这种方法，如同大多数传统的方法一样，忽略了词语的语义和语法层面的信息与关系。

\citep{rumelhart1988learning}第一次提出了词向量的概念。词向量是一种类似于one-hot向量的使用向量来表示词语的方法。但与one-hot向量所不同的是，词向量没有将向量限制为某种特殊的向量，而是使用维度不高的实数空间上的一般向量来表示词语。这种做法较one-hot向量而言，具有较小的向量维度。同时，由于词向量不是由人工设定的方式生成的，而是根据文本的特点通过学习自动生成的，词向量也会携带一部分词语的语义与语法信息。这些特性，使得词向量的概念一经提出，就迅速被广泛运用到神经网络语言模型等许多自然语言处理的问题中，并取得了可观的成功\citep{collobert2008unified,schwenk2007continuous}。

几年前，Google 发布了著名的\emph{word2vec}工具包，这个工具包训练得到的词向量不光具有维度低的特点，还具有一些非常有趣的特性\citep{mikolov2013efficient,mikolov2013distributed,mikolov2013linguistic}。一方面，\emph{word2vec}可以在较大量的语料上以较高效率学习并获得具有较高质量的词向量。这种高效性使得它可以被广泛地运用在各种模型中。除此之外，更引人注意的是，通过这个模型学习出的词向量可以通过向量的加减运算来表示词语间的语义语法关系。比如说，如果我们用$\vvec[\mbox{词语}]$表示一个词语所对应的词向量，我们会发现存在如下关系：$\vvec[\mbox{man}]-\vvec[\mbox{woman}] \approx \vvec[\mbox{king}] - \vvec[\mbox{queen}]$，$\vvec[\mbox{Chinese}] + \vvec[\mbox{river}] \approx \vvec[\mbox{Yangtze\_River}]$。即，男人与女人所对应的词向量之差会近似等于国王与王后对应词向量之差，中国与河流对应词向量之和会近似等于长江的词向量。同时，男人与女人间的语义关系与国王与王后间的语义关系确实有较大的相似性，而长江的语义也可以看做中国和河流两个词语词义的叠加。这一现象表示\emph{word2vec} 所学习获得的词向量是可以较好得表示词语之间语义或语法的关系的。

由于这些特性，在\emph{word2vec}提出后，迅速引起了广泛的关注，同时也启发了许多对这一模型进行解释、改进的研究工作\citep{li2015word,levy2014neural,pennington2014glove,huang2012improving}。这些工作中，有些对\emph{word2vec}的数学原理进行了探讨，为\emph{word2vec}方法提供了理论的依据，例如\citep{li2015word,levy2014neural}证明\emph{word2vec}的学习过程近似等价于对由词语同时出现的次数所的构成的某种信息矩阵进行矩阵分解。有些工作讨论\emph{word2vec}所学习得到的词向量之间的关系可以通过代数运算表示的原因，并在此基础上提出了新的模型\citep{pennington2014glove}。还有些工作，针对\emph{word2vec}的模型不能很好的处理多义词的现象，对这个模型进行了语义上的强化\citep{huang2012improving}。

下面，我们先对之前的一部分经典工作进行总结，然后再在他们的基础上进一步解释为什么可以使用词向量间的代数运算表示词语语义与语法关系，同时针对多义词的语义数目往往有所不同的现象对词向量学习的算法进行了进一步的改进，并通过实验验证了这些改进的有效性与可行性。