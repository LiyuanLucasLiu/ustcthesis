\iffalse

\bibliography{..\\bib\\tex.bib}

\fi

\chapter{相关工作}

在这一节中，我们先介绍一系列使用数学工具对\emph{word2vec}模型进行建模的工作，然后介绍针对多义词现象所对模型进行的改进。

\section{\emph{word2vec}与矩阵分解之间的关系}
\label{sec:word2vecmf}

\emph{word2vec}的优势之一，在于它具有较快的数据处理速度，可以在大量的数据上进行运算。而为了获得较高的计算效率，\emph{word2vec}对许多运算进行了近似与简化（如在\ref{subsubsec:HS}与\ref{subsubsec:NS}中所介绍的Hierarchical Softmax与Negative Sampling），这些近似与简化大大提高了模型的计算效率。同时也有许多工作对简化后的模型进行了分析与讨论，为简化后的模型提供了理论上的支持。

在2014年，有学者证明，\emph{word2vec}简化后的模型实际上是对矩阵进行隐式的矩阵分解\citep{levy2014neural}，例如\emph{word2vec}中的skip-gram negative sampling模型可以解释为对逐点互信息矩阵进行分解。这里之所以称为隐式的矩阵分解是因为，在证明中，\emph{word2vec}的模型只有在很理想的条件下才能与矩阵分解等价，而现实数据往往是无法满足这些条件的。另一方面，直接对信息矩阵进行矩阵分解后所得到词向量质量比原始\emph{word2vec}模型所得到的要低。\citep{li2015word}提出了一种方法弥补了这些缺陷，为\emph{word2vec}提供更坚实的理论基础。他证明了skip-gram negative sampling模型等价于某种构造出的矩阵分解，同时，对这个矩阵直接进行矩阵分解后，得到了同\emph{word2vec}一样高质量的词向量。也就是说，\emph{word2vec}可以直接被显式地解释为矩阵分解。

\section{对\emph{word2vec}的几种改进}
\label{sec:intro_poly}

在原始的\emph{word2vec}中，字典中的每一个词语都会被表示为一个唯一的词向量。这种做法虽然简洁，也是具有一些缺点的，例如，很大一部分的词语都具有超过一种语义，例如Apple既可以表示一种水果，也可以表示一家科技公司，对于这些词语，如果依然简单地使用一个词向量对多种不同的语义进行表示，则会造成多个语义间的混淆。为了克服这个缺点，我们可以让每个词语对应多个词向量，例如可以让Apple对应两个词向量，其中一种表示一种水果，另一种表示一家科技公司。\citep{huang2012improving}通过借用机器学习中传统聚类方法K-Means算法\citep{macqueen1967some}，先对多义词的不同词义进行识别，然后按照识别出的语义，将同一个单词的不同词义当作不同的单词进行传统的词向量学习。如对于单词Apple，则可能会被识别出具有两种语义，并将表示水果时的Apple当作Apple-1处理，将表示科技公司时的Apple当作Apple-2处理。这种方法可以初步解决多义词现象，但它没有将语义的识别与词向量的学习进行了结合。\citep{tian2014probabilistic}提出了一个概率模型框架将多义词的识别与判断进行统一地求解，并获得了不错的实验结果。

另一方面，\citep{pennington2014glove}分析了\emph{word2vec}获得的词向量可以表示部分语法与语义信息的背后的原理与机制，并在此基础上设计模型利用全局信息进行词向量的学习，获得了不错的实验结果。\citep{huang2012improving}在使用多个向量来解决多义词问题时，也使用了全局信息来进一步提升的学习到的词向量的质量。