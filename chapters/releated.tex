\iffalse

\bibliography{..\\bib\\tex.bib}

\fi

\chapter{相关工作}

\emph{word2vec}的模型在被提出后，引起了很大的关注，也启发许多研究者开展了相关的分析与改进。在这里，我们先介绍一系列使用数学工具对\emph{word2vec}模型进行建模的工作，然后介绍针对多义词现象所进行的模型改进与应用。

\section{\emph{word2vec}与矩阵分解之间的关系}
\label{sec:word2vecmf}

\emph{word2vec}能获得成功的原因之一，是他具有较快的数据处理速度，可以在大量的数据上进行运算。为了获得较高的计算效率，\emph{word2vec}对许多运算进行了简化近似，这些简化与近似大大提高了模型的计算效率，但却由于缺少理论上的分析，使得改进后的模型的数学原理并不清晰。

在2014年，有学者证明，这些简化后的模型实际上是对矩阵进行隐式的矩阵分解\citep{levy2014neural}，例如\emph{word2vec}中的skip-gram negative sampling模型可以解释为对逐点互信息矩阵进行分解。这里之所以称为隐式的矩阵分解是因为，\emph{word2vec}的模型只有在很理想的条件下才与矩阵分解等价，而在实际数据中，往往无法满足这样的理想条件。另一方面，如果直接在论文所提到的矩阵上进行矩阵分解，例如逐点互信息矩阵，实际上是得不到和\emph{word2vec}一样具有高质量的词向量的。为了弥补这些缺陷，为\emph{word2vec}提供更坚实的理论基础，\citep{li2015word}提出了一种方法，使得矩阵分解完全等价于skip-gram negative sampling模型，同时，对这个矩阵直接进行矩阵分解后，得到了同\emph{word2vec}一样高质量的词向量。这说明\emph{word2vec}可以直接被解释为显式的矩阵分解。

\section{对\emph{word2vec}的改进}
\label{sec:intro_poly}

在原始的\emph{word2vec}中，对于每一个词语都会有一个唯一的词向量与之对应。这种做法虽然非常简洁，但也有一些缺点。例如有很大一部分词语，都具有超过一种的意义，如Apple既可以表示一种水果，也可以表示一家科技公司，对于这些词语，如果依然简单地使用一个词向量同时表示他的多种意义，是不恰当的。为了克服这个缺点，我们可以让每个词语对应多个词向量，例如可以让Apple对应两个词向量，其中一种表示一种水果，另一种表示一家科技公司。\citep{huang2012improving}通过借用机器学习中传统聚类方法K-Means\citep{macqueen1967some}，先对多义词进行识别，然后对同一个单词的不同意义当作不同的单词进行传统的词向量学习。例如将表示水果时的Apple当作Apple-1，将表示科技公司时的Apple当作Apple-2。这种方法虽然简单，但也确实可以初步解决多义词现象。在此基础上，\citep{tian2014probabilistic}提出了一个概率模型框架将多义词的识别与判断进行统一地求解，并获得了不错的实验结果。

另一方面，\citep{pennington2014glove}分析了\emph{word2vec}获得的词向量可以表示部分语法与语义信息的原因，同时在分析的基础上设计模型利用全局信息进行词向量的学习，获得了不错的实验结果。\citep{huang2012improving}在使用多个向量来解决多义词问题时，也使用了全局信息来进一步提升的学习到的词向量的质量。